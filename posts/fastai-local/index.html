<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Attempt to run fast.ai notebooks locally</title>
    <link rel="stylesheet" href="/static/css/style.css" />
    <link rel="stylesheet" href="/static/css/prism.css" />
    <link rel="icon" type="image/png" href="/static/favicon/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/svg+xml" href="/static/favicon/favicon.svg" />
    <link rel="shortcut icon" href="/static/favicon/favicon.ico" />
    <link rel="apple-touch-icon" sizes="180x180" href="/static/favicon/apple-touch-icon.png" />
    <link rel="manifest" href="/static/favicon/site.webmanifest" />
  </head>

  <body>
    <header>
      <nav>
        <a class="relative" href="/">Home</a>
        <a class="relative" href="/posts">Posts</a>
        <a class="relative" href="/projects">Projects</a>
        <a class="relative" href="/static/resume-public.pdf">Resume</a>
        <a class="relative" href="/sitemap">Sitemap</a>
      </nav>
      <div>
        <a href="https://github.com/mayajosyula">GitHub</a>
        <a href="https://www.linkedin.com/in/maya-josyula">LinkedIn</a>
      </div>
    </header>
    <main>
      <div class="post-header">
        <h1>Attempt to run fast.ai notebooks locally</h1>
        April 7, 2021

        <div class="box-tag">
          <a href="/tags/recursecenter">recursecenter</a>

          <a href="/tags/fastai">fastai</a>

          <a href="/tags/pytorch">pytorch</a>

          <a href="/tags/gpu">gpu</a>
        </div>
      </div>

      <p>
        These are some notes from my attempt to install fastai and fastbook locally and run the Jupyter notebooks from
        the fast.ai course using my laptop, instead of using a service like Paperspace Gradient or Google Colab. It
        didn’t completely work, so I might try again later.
      </p>
      <p><strong>Summary</strong></p>
      <ul>
        <li>It is possible to install fastai on Windows 10.</li>
        <li>It is possible to get PyTorch to recognize the GPU on a laptop.</li>
        <li>
          A 2GB GPU does not appear to be enough memory to run fast.ai notebooks locally, even if you clear cache
          beforehand, but this can be mitigated by reducing batch size.
        </li>
        <li>
          The laptop may be using the CPU whenever it’s available, even if you try to force it to use the GPU with
          <code>learn.model.cuda()</code> (but I’m not sure about this).
        </li>
      </ul>
      <h2>Why I was motivated to try this</h2>
      <ul>
        <li>I wanted to do the <a href="https://course.fast.ai/">fast.ai course</a></li>
        <li>Getting GPU instances on Paperspace Gradient is annoyingly difficult unless you pay</li>
        <li>My laptop has a NVIDIA graphics card</li>
        <li>
          Even if it doesn’t work in the end, being able to run PyTorch locally on a GPU might be useful in general.
        </li>
      </ul>
      <h3>What I had originally</h3>
      <ul>
        <li>Windows 10</li>
        <li>Anaconda 3</li>
        <li>Python version: 3.8.5</li>
        <li>PyTorch version: 1.8.1</li>
      </ul>
      <p>
        I ended up having to downgrade my PyTorch version anyway in order for fastai to work properly, so having
        Anaconda/Python was the only important thing.
      </p>
      <p>
        My laptop’s graphics card is a <strong>NVIDIA GeForce MX150</strong> - found this out by launching Device
        Manager and clicking on “Display adapters”.
      </p>
      <h2>Installation</h2>
      <h3>CUDA</h3>
      <p>
        <a
          href="https://developer.nvidia.com/cuda-10.2-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exelocal"
          >Here is the page</a
        >
        for CUDA version 10.2, which is a 2.6GB exe file. (There are also two patches that need to be installed on top
        of that.)
      </p>
      <p>
        <strong>Visual Studio</strong> Looking around, it seems like NVIDIA CUDA requires Microsoft Visual Studio. So I
        ended up installing the free Community edition of Visual Studio, just the core editor with no add-ons. I had
        already installed the Visual Studio build tools a while ago for something else — maybe the build tools were
        really all I needed, but I thought I’d play it safe.
      </p>
      <p>
        First installed the “Express” version of CUDA, which was the recommended one, then installed the two patches. It
        was pretty straightforward and went the same way installing things usually works on Windows (download exe file,
        launch installer, etc).
      </p>
      <h3>cuDNN</h3>
      <p>
        Getting cuDNN requires participation in the NVIDIA Developer Program, which needs a NVIDIA account. After
        clicking on a lot of pictures of trucks to prove I’m human and verifying my email, there was a page that asked
        for my name and organization details, and then at the bottom is a checkbox that signs you up for the NVIDIA
        Developer Program if you select it.
      </p>
      <p>
        <a href="https://developer.nvidia.com/rdp/cudnn-download">Here is the download page</a> for cuDNN. The download
        is a .zip file, and if you extract it and open it up, you’ll find three subdirectories: <code>bin</code>,
        <code>include</code>, and <code>lib</code>, each with a bunch of .dll files in them.
      </p>
      <p>
        These files need to go into the CUDA installation. There is probably an efficient way to do this (maybe
        involving path variables or something?) but I went with the manual approach. If you navigate to where your CUDA
        files are (for me it was <code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2</code>, which is the
        default) you will see a bunch of folders. Three of them are <code>bin</code>, <code>include</code>, and
        <code>lib</code>, and you can just copy all the .dll files from cuDNN’s <code>bin</code> folder into CUDA’s
        <code>bin</code> folder, then do the same with <code>include</code> and <code>lib</code>.
      </p>
      <p>That seems to be all you have to do.</p>
      <h3>PyTorch</h3>
      <p>
        It seems like (at least on Windows 10) fast.ai doesn’t support the newest stable PyTorch version. So I had to
        downgrade to version 1.7.1. This was pretty easy to do:
      </p>
      <pre
        class="language-bash"><code class="language-bash">conda <span class="token function">install</span> <span class="token assign-left variable">pytorch</span><span class="token operator">==</span><span class="token number">1.7</span>.1 <span class="token assign-left variable">torchvision</span><span class="token operator">==</span><span class="token number">0.8</span>.2 <span class="token assign-left variable">torchaudio</span><span class="token operator">==</span><span class="token number">0.7</span>.2 <span class="token assign-left variable">cudatoolkit</span><span class="token operator">=</span><span class="token number">10.2</span> <span class="token parameter variable">-c</span> pytorch</code></pre>
      <h3>fastai</h3>
      <p>
        Surprisingly enough, this was also pretty easy to do - probably because all the dependencies were taken care of
        beforehand. Installing <code>jupyter_contrib_nbextensions</code> was mostly just to be safe because of all the
        Jupyter notebooks — it might not have been necessary.
      </p>
      <pre
        class="language-bash"><code class="language-bash">pip <span class="token function">install</span> jupyter_contrib_nbextensions
pip <span class="token function">install</span> fastai</code></pre>
      <p>No errors! :)</p>
      <h2>Testing it out</h2>
      <h3>PyTorch is recognizing the GPU!</h3>
      <p>You can check this with the Python interpreter.</p>
      <pre
        class="language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> torch
<span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token boolean">True</span>
<span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>current_device<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token number">0</span>
<span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token number">1</span>
<span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_device_name<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token string">'GeForce MX150'</span></code></pre>
      <h3>Launching the fast.ai course notebooks</h3>
      <p>
        The course notebooks can be found on the
        <a href="https://github.com/fastai/fastbook">fastai book GitHub page</a>. I downloaded the repository as a .zip
        file, then just launched Jupyter Notebooks and opened the notebook files up, and they seemed to display fine.
      </p>
      <h3>Installing fastbook and graphviz</h3>
      <p>
        Selecting the very first cell of Lesson 1 and trying to run it immediately gave an error saying something along
        the lines of “missing <code>graphviz</code> — please run <code>conda install fastbook</code>”.
      </p>
      <p>
        This was really misleading, because running <code>conda install fastbook</code> and variations of that ended up
        giving me errors. The <a href="http://forums.fast.ai/">fast.ai forums</a> has a bunch of threads discussing it,
        all with different fixes that ended up working for different people? Maybe it’s a computer-specific thing. The
        fix that worked for me was adding <code>!pip install graphviz</code> to the second line of the Jupyter notebook.
        So now the first cell looks like:
      </p>
      <pre class="language-python"><code class="language-python"><span class="token comment">#hide</span>
!pip install <span class="token operator">-</span>Uqq fastbook
!pip install graphviz
<span class="token keyword">import</span> fastbook
fastbook<span class="token punctuation">.</span>setup_book<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
      <p>Then running the first and second cells worked without any problems.</p>
      <h2>Cat and dog-related issues (Lesson 1)</h2>
      <p>
        The next thing I did was skip to the part of the notebook that actually trains a model, which is the cat/dog
        classification system. Running that cell also threw a bunch of errors.
      </p>
      <h3>Issue 1: found at least two devices, cuda:0 and cpu (solved)</h3>
      <p>
        Not entirely sure what causes this error, but it seems like it’s happening because some tensors are stored on
        the CPU and some are stored on the GPU, so not all of the data is accessible? I’m not sure, though.
      </p>
      <p>
        One of the forum threads had a fix for this. It involves changing the <code>fastai</code> code, which is pretty
        ridiculous but it does work? If you navigate to the fastai library (for me it was under
        <code>anaconda3\Lib\site-packages\fastai</code>), and then go to the <code>data</code> folder and open the file
        load.py, there is a function called <code>__iter__</code> on line 105:
      </p>
      <pre
        class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>randomize<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>before_iter<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>__idxs<span class="token operator">=</span>self<span class="token punctuation">.</span>get_idxs<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># called in context of main process (not workers/subprocesses)</span>
    <span class="token keyword">for</span> b <span class="token keyword">in</span> _loaders<span class="token punctuation">[</span>self<span class="token punctuation">.</span>fake_l<span class="token punctuation">.</span>num_workers<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>fake_l<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># fix issue 2899. If the process start method isn't fork, the data will be copied to cuda in learner one_batch.</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>device <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> multiprocessing<span class="token punctuation">.</span>get_start_method<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"fork"</span><span class="token punctuation">:</span>
            b <span class="token operator">=</span> to_device<span class="token punctuation">(</span>b<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token keyword">yield</span> self<span class="token punctuation">.</span>after_batch<span class="token punctuation">(</span>b<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>after_iter<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">'it'</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">del</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>it<span class="token punctuation">)</span></code></pre>
      <p>
        You need to get rid of the <code>multiprocessing.get_start_method().lower() == &quot;fork&quot;</code> in the
        first if-statement. So that bit turns into:
      </p>
      <pre
        class="language-python"><code class="language-python">    <span class="token keyword">for</span> b <span class="token keyword">in</span> _loaders<span class="token punctuation">[</span>self<span class="token punctuation">.</span>fake_l<span class="token punctuation">.</span>num_workers<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>fake_l<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>device <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            b <span class="token operator">=</span> to_device<span class="token punctuation">(</span>b<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token keyword">yield</span> self<span class="token punctuation">.</span>after_batch<span class="token punctuation">(</span>b<span class="token punctuation">)</span></code></pre>
      <p>
        Not sure what to think about this “fix”. It’s essentially treating this as a bug with fastai instead of a
        problem with the way things were configured. Which I’m not convinced is the right way to be debugging code. For
        now though, I just went ahead and changed it.
      </p>
      <h3>Issue 2: CUDA out of memory error (partially solved)</h3>
      <p>
        Fixing the above problem instantly resulted in this one. In one way it’s kind of reassuring, because it means
        the notebook is clearly trying to use the GPU in some way. But it’s also annoying, because I can’t find an easy
        way to give myself more memory.
      </p>
      <p>
        The graphics card had 2 GB of memory, and the consensus on the forums seems to be that that’s not enough. I
        tried restarting the kernel and also tried running <code>torch.cuda.empty_cache()</code> in the hope that
        clearing the cache would help, but that didn’t free up enough.
      </p>
      <p>
        Then I learned about batch size. Apparently it’s a parameter for the number of training examples that are used
        in one iteration, and it defaults to 64. You can adjust it to whatever you want by setting <code>bs</code> when
        reading in the dataset, and smaller batch sizes use less memory:
      </p>
      <pre
        class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">is_cat</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">return</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>isupper<span class="token punctuation">(</span><span class="token punctuation">)</span>
    dls <span class="token operator">=</span> ImageDataLoaders<span class="token punctuation">.</span>from_name_func<span class="token punctuation">(</span>
    path<span class="token punctuation">,</span> get_image_files<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">,</span> valid_pct<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span>
    label_func<span class="token operator">=</span>is_cat<span class="token punctuation">,</span> item_tfms<span class="token operator">=</span>Resize<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span> bs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span> <span class="token comment"># bs = batch size</span></code></pre>
      <p>
        Just keep halving the values until there are no longer out of memory errors. It ended up working for me with a
        batch size of 8, though it might be good to fine-tune that a little more if possible.
      </p>
      <h3>Issue 3: CPU is being used instead of GPU according to Task Manager (needs investigation)</h3>
      <p>
        After changing the batch size, the cat/dog cell does run! It takes a while though — close to 10-15 minutes. I’m
        not sure if it’s solely a hardware problem, a low batch size problem, or if the laptop is using the CPU instead
        of the GPU.
      </p>
      <p>
        I tried rerunning the cat/dog cell and simultaneously opened up Task Manager, which said that Python’s usage was
        somewhere around 40% CPU and 0.1% GPU. If that’s accurate, it seems like the GPU on the laptop is kind of
        “overflow” for the CPU in some way? In that if a task uses too much processing power, the system migrates to
        using the GPU automatically, but uses the CPU whenever it can. Maybe that’s incorrect though, and/or Task
        Manager isn’t accurate.
      </p>
      <p>
        The fast.ai forums say that you can force the model to use the GPU by calling
        <code>learn.model.cuda()</code> beforehand. This didn’t appear to change anything, though. The entire cell looks
        like this:
      </p>
      <pre class="language-python"><code class="language-python"><span class="token comment">#id first_training</span>
<span class="token comment">#caption Results from the first training</span>
<span class="token comment"># CLICK ME</span>
<span class="token keyword">from</span> fastai<span class="token punctuation">.</span>vision<span class="token punctuation">.</span><span class="token builtin">all</span> <span class="token keyword">import</span> \<span class="token operator">*</span>
path <span class="token operator">=</span> untar_data<span class="token punctuation">(</span>URLs<span class="token punctuation">.</span>PETS<span class="token punctuation">)</span><span class="token operator">/</span><span class="token string">'images'</span>

<span class="token keyword">def</span> <span class="token function">is_cat</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">return</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>isupper<span class="token punctuation">(</span><span class="token punctuation">)</span>
    dls <span class="token operator">=</span> ImageDataLoaders<span class="token punctuation">.</span>from_name_func<span class="token punctuation">(</span>
    path<span class="token punctuation">,</span> get_image_files<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">,</span> valid_pct<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span>
    label_func<span class="token operator">=</span>is_cat<span class="token punctuation">,</span> item_tfms<span class="token operator">=</span>Resize<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span> bs<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>

learn <span class="token operator">=</span> cnn_learner<span class="token punctuation">(</span>dls<span class="token punctuation">,</span> resnet34<span class="token punctuation">,</span> metrics<span class="token operator">=</span>error_rate<span class="token punctuation">)</span>
learn<span class="token punctuation">.</span>model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># this is the added line</span>
learn<span class="token punctuation">.</span>fine_tune<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre>
      <h2>Places to go from here?</h2>
      <p>There are three options as far as I can tell:</p>
      <ul>
        <li>Give up and go back to Paperspace Gradient, or</li>
        <li>keep trying to get this to work, or</li>
        <li>keep going through the rest of the lessons with the settings I have, and see how far I get.</li>
      </ul>
      <p>
        Don’t really know how to fix the memory issues, aside from paying for a better GPU (not ideal). Using smaller
        batch sizes did seem like it worked for Lesson 1 at least, but future lessons will probably have larger models
        so it’s likely not a long-term fix.
      </p>
      <p>
        The CPU vs GPU problem should be fixable, though — I think I just need to figure out the right
        settings/functions for it. Hopefully continuing to search the fast.ai forums will eventually yield an
        alternative solution.
      </p>
      <p>
        Either way, I did learn a lot from attempting to do this, and I also gained a new appreciation for how useful
        the fast.ai forums are — there are so many people who have taken the course that the chances of someone else
        running into your exact issue are pretty high!
      </p>
      <p><strong>References:</strong></p>
      <p>
        <a href="https://lazyprogrammer.me/how-to-setup-nvidia-gpu-laptop-with-ubuntu-for-deep-learning-cuda-and-cudnn/"
          >https://lazyprogrammer.me/how-to-setup-nvidia-gpu-laptop-with-ubuntu-for-deep-learning-cuda-and-cudnn/</a
        >
      </p>
      <p>
        <a
          href="https://towardsdatascience.com/intro-to-fastai-installation-and-building-our-first-classifier-938e95fd97d3"
          >https://towardsdatascience.com/intro-to-fastai-installation-and-building-our-first-classifier-938e95fd97d3</a
        >
      </p>
      <p>
        <a
          href="https://towardsdatascience.com/an-utterly-simple-guide-on-installing-tensorflow-gpu-2-0-on-windows-10-198368dc07a1"
          >https://towardsdatascience.com/an-utterly-simple-guide-on-installing-tensorflow-gpu-2-0-on-windows-10-198368dc07a1</a
        >
      </p>
      <p>
        <a href="https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu"
          >https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu</a
        >
      </p>
      <p>
        <a href="https://forums.fast.ai/t/resolving-issues-with-fastai-on-windows-10-anaconda-due-to-torch-1-8/86875"
          >https://forums.fast.ai/t/resolving-issues-with-fastai-on-windows-10-anaconda-due-to-torch-1-8/86875</a
        >
      </p>
      <p>
        <a href="https://forums.fast.ai/t/missing-graphviz-please-run-conda-install-fastbook/77260/24"
          >https://forums.fast.ai/t/missing-graphviz-please-run-conda-install-fastbook/77260/24</a
        >
      </p>
      <p>
        <a href="https://forums.fast.ai/t/part1-2020-02-production-found-at-least-two-devices-cuda-0-and-cpu/87253"
          >https://forums.fast.ai/t/part1-2020-02-production-found-at-least-two-devices-cuda-0-and-cpu/87253</a
        >
      </p>
      <p>
        <a href="https://forums.fast.ai/t/cuda-out-of-memory/45499/4"
          >https://forums.fast.ai/t/cuda-out-of-memory/45499/4</a
        >
      </p>
      <p>Created while participating at the <a href="https://www.recurse.com/">Recurse Center</a>!</p>
    </main>

    <footer>
      <hr />
      <p>All views expressed here are my own.</p>
      <p><a href="https://www.11ty.dev/">Made with Eleventy</a></p>
    </footer>
  </body>
</html>
